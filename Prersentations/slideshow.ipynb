{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MEC 560: Linear Algebra Review\n",
    "## Vivek Yadav\n",
    "## 313 Frey Hall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "- Vectors and vector operations\n",
    "    - Sum/difference\n",
    "    - Inner/outer products\n",
    "- Matrix operations\n",
    "- Eigen values and eigen vectors\n",
    "- Singular value decompositions\n",
    "- System of equations\n",
    "- Matrix derivatives\n",
    "- Least squares\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors \n",
    "Vectors are scientific quantities that have both direction and magnitude. Vectors can be as simple as a line segment defined on a 2-D Euclidean plane, or a multi-dimensional collection of numerical quantities. \n",
    "\n",
    "### Sum/difference of vectors\n",
    "Sum/difference of vectors is the vector of sum/differences\n",
    "\n",
    "$$ \\overrightarrow {a} =  \\left[  1, 2, 3 \\right] $$ \n",
    "$$ \\overrightarrow {b} =  \\left[ 3, 3, 4 \\right] $$\n",
    "\n",
    "$$ \\overrightarrow {a}+\\overrightarrow {b} = \\left[ 1+3, 2+3, 3+4 \\right] = \\left[ 4, 5, 7 \\right].$$\n",
    "\n",
    "$$ \\overrightarrow {a}-\\overrightarrow {b} = \\left[ 1-3, 2-3, 3-4 \\right] = \\left[ -2, -1, -1 \\right].$$\n",
    "\n",
    "***  Vector sum (or difference) is defined only between the vectors of the same dimension. ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Inner Product\n",
    "Inner product (also dot product) of two vectors \\\\(\\overrightarrow {a}  \\\\) and \\\\( \\overrightarrow {b}\\\\) is defined as, \n",
    "\n",
    "$$  \\overrightarrow {a} \\circ  \\overrightarrow {b} = \\overrightarrow {a}^T  \\overrightarrow {b} = \\sum_{i=1}^n a_i b_i, $$\n",
    "\n",
    "where \\\\(n\\\\) is the length of the vectors \\\\(\\overrightarrow {a}  \\\\) and \\\\( \\overrightarrow {b}\\\\). \n",
    "\n",
    "1. Inner product represents how similar two vectors are to one another. This is also referred as Cosine similarity.  \n",
    "$$ cos(\\theta) =  \\frac{\\overrightarrow {a} \\circ  \\overrightarrow {b}}{ | \\overrightarrow {a}|  |\\overrightarrow {b}|} ,$$\n",
    "2. Inner product also represents the length of the projection of one vector along the other vector. Therefore, the projection of vector \\\\(\\overrightarrow {a}\\\\) along vector \\\\(\\overrightarrow {b}\\\\) is\n",
    "$$ \\frac{\\overrightarrow {a} \\circ  \\overrightarrow {b}}{ |\\overrightarrow {b}|} .$$\n",
    "\n",
    "***  Inner product is defined only between the vectors of the same dimension. ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outer Product: \n",
    "\n",
    "Outer product of two vector \\\\( \\overrightarrow {a}  \\\\) and \\\\( \\overrightarrow {b}\\\\) is the matrix \\\\(C_{ij}\\\\) such that\n",
    "\n",
    "$$ C_{ij} = a_i b_j$$ \n",
    "\n",
    "Outer product is helpful in certain applications to reduce computation and storage requirements.  \n",
    "\n",
    "***  Outer product is defined between any two vectors and has no contraint on the length of either vector. ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "A matrix (plural matrices) is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. In control systems, matrices are used to describe how the state vectors (for example position and velocity) evolve and how the control signal influences the state vectors. A matrix with \\\\(n\\\\)  rows and \\\\(m\\\\)  columns has a dimension of \\\\( n \\times m\\\\)  matrix and is represented as \\\\(A_{n \\times m}\\\\) .\n",
    "\n",
    "#### Sum/difference of matrices: \n",
    "Sum/difference) of two matrices is equal to the matrix formed by sum (or difference) of individual elements, and has the same dimension as the original matrices. For example, sum of matrices \\\\( A = a_{i,j}\\\\) and \\\\( B = b_{i,j}\\\\) is\n",
    "\n",
    "$$ C =c_{i,j} = a_{i,j} + b_{i,j}  . $$ \n",
    "\n",
    "***  Matrix sum (or difference) is defined only between the vectors of the same dimension. ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Product of matrices:\n",
    "\n",
    "\n",
    "##### Product of matrix and scalar\n",
    "Product of a matrix with a scalar is equal to the matrix formed by multiplying each element of the matrix by the scalar.  Therefore, \n",
    "\n",
    "$$ \\lambda A = \\lambda a_{i,j} $$\n",
    "\n",
    "##### Product of matrix and matrix\n",
    "\n",
    "\n",
    "Product of matrices , \\\\( A B \\\\) between two matrices  $A_{n \\times p}$  and $B_{p \\times m} $  as, \n",
    "\n",
    "$$ C_{n \\times m} = A_{n \\times p} B_{p \\times m} = c_{i,j} = \\sum_{k=1}^{p} a_{i,k} b_{k,j}, $$\n",
    "\n",
    "The matrix product is defined only if the number of columns in \\\\(A\\\\) is equal to the number of rows in \\\\(B\\\\). (\\\\(C\\\\)) is of dimension \\\\( n \\times m \\\\). \n",
    "\n",
    "***Note:*** Outer product can also be used to compute matrix product, and typically results in lower computation and storage requirements. \n",
    "\n",
    "$$ C_{n \\times m} =  \\sum_{k=1}^{p} A_{(i,:)}  B_{(:,j)}, $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "##### Product of matrix and vector (column space or range)\n",
    "\n",
    "Product of a matrix with a vector is defined in a similar manner as above, however in the special case of multiplying a matrix by vector results in linear combination of the columns.    \n",
    "\n",
    "$$ A_{n\\times m} b_{m \\times 1} = \\left[ A_{(:,1)} A_{(:,2)} ... A_{(:,m)} \\right] b_{m \\times 1}$$\n",
    "\n",
    "Expanding \\\\(b_{m \\times 1}\\\\) as \\\\([b_1 b_2 ... b_m]^T\\\\) and multiplying gives, \n",
    "\n",
    "$$ A_{n\\times m} b_{m \\times 1} = b_1 A_{(:,1)}+ b_2 A_{(:,2)}+ ... + b_m A_{(:,m)} $$\n",
    "\n",
    "Therefore, the product of $ A_{n\\times m}$ and $b_{m \\times 1}$ is a linear combination of the columns of \\\\(A\\\\) and the weighing factors are determined by the vector $b_{m \\times 1}$. The space spanned by the columns of \\\\(A\\\\) is also called the column space or range of the matrix \\\\(A\\\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Product of vector and matrix (row space)\n",
    "\n",
    "Product of a vector with a matrix results in linear combination of the rows.    \n",
    "\n",
    "$$ a_{1\\times n} B_{n \\times m} = a_{1 \\times n} \\left[ \\begin{array}{c}\n",
    "B_{(1,:)}  \\\\\n",
    "B_{(2,:)}   \\\\\n",
    "\\vdots \\\\\n",
    "B_{(n,:)}   \\\\\n",
    "\\end{array} \\right] $$\n",
    "\n",
    "Expanding \\\\(a_{1 \\times n}\\\\) as \\\\([a_1 a_2 ... a_n]\\\\) and multiplying gives, \n",
    "\n",
    "$$ a_{1\\times n} B_{n \\times m} = a_1 B_{(1,:)}+ a_2 B_{(2,:)}+ ... + a_n B_{(n,:)} $$\n",
    "\n",
    "Therefore, the product of $ a_{1\\times n}$ and $B_{n \\times m}$ is a linear combination of the rows of \\\\(B\\\\) and the weighing factors are determined by the vector \\\\(a_{1 \\times n}\\\\). The space spanned by the rows of \\\\(B\\\\) is also called the row space or range of the matrix \\\\(B\\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Rank of a matrix\n",
    "The rank of the matrix is equal to the number of independent rows (or columns). Note, column and row ranks of a matrix are equal. Check the [wikipage](http://tinyurl.com/zkouz23) for a brief description of why. \n",
    "\n",
    "#### c) Determinant of a matrices:\n",
    "\n",
    "Determinant, defined for square matrices is an estimate of the 'size' of a matrix. Determinants represent the volume enclosed by the rows of the matrix in an n-dimensional hyperspace. For a 2-dimensional matrix, the determinant represents area enclosed by the vectors composed of the rows (or columns) of the matrix. \n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/parallelogram.png\",width=200, height=200)>\n",
    "  <figcaption> Fig1. - Determinant of a 2-dimension matrix is equal to the area of the parallelogram enclosed by the rows (or columns) of the matrix.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Inverse of matrices:\n",
    "\n",
    "Inverse of a square matrix is the matrix that when multiplied by the original matrix gives an identity matrix. Identity matrix is a matrix that has 1s for diagonal terms and 0 for all the off-diagonal terms. Inverse of a matrix is computed as the matrix of cofactors divided by the determinant of the matrix, details [here](http://tinyurl.com/jyz8dox). Cofactors of an element \\\\( i,j \\\\) of a matrix is the matrix formed by removing \\\\(i^{th} \\\\) column and \\\\(j^{th} \\\\) column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Matrix inverse example\n",
    "\n",
    "Compute the inverse of \n",
    "\n",
    "$$ A = \\left[ \\begin{array}{ccc}  1 & 2 &  3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9  \\end{array} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix inverse example\n",
    "\n",
    "1. Compute minors:  Minor of an element at \\\\( (i,j)\\\\) position is taken by removing \\\\( i^{th}\\\\) row and \\\\(j^{th}\\\\) columns. \n",
    "2. Compute cofactor:  Cofactor of an element at \\\\( (i,j)\\\\) position is taken by computing the Minor and then multiplying it with \\\\( (-1)^{i*j} \\\\)\n",
    "3. Compute inverse by multiplying cofactors of \\\\( (i,j)^{th}\\\\) element by the element and add along any row (or column). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Eigen values and eigen vectors:\n",
    "\n",
    "Consider a vector \\\\(\\overrightarrow{v}\\\\) such that \n",
    "\n",
    "$$ A \\overrightarrow{v} = \\lambda \\overrightarrow{v},$$\n",
    "\n",
    "i.e. product of the matrix \\\\(A\\\\) times the vector \\\\(\\overrightarrow{v}\\\\) is the vector \\\\(\\overrightarrow{v}\\\\) scaled by a factor of \\\\(\\lambda\\\\). The vector \\\\(\\overrightarrow{v}\\\\) is called the Eigen vector and the multiplier \\\\(\\lambda\\\\) is called the eigen value. Eigen values are first computed by solving the characteristic equation of the matrix \n",
    "\n",
    "$$ det( A - \\lambda I) = 0, $$\n",
    "\n",
    "and next the first equation \\\\( A \\overrightarrow{v} = \\lambda \\overrightarrow{v}\\\\) is used to obtain non-trival solution of eigen vectors. Eigen vectors are scaled so their magnitude is equal to 1. \n",
    "\n",
    "If \\\\(V\\\\) is the matrix of all the eigen vectors then,\n",
    "\n",
    "$$ AV = V \\Lambda$$\n",
    "\n",
    "where \\\\(\\Lambda\\\\) is the matrix whose off-diagonal terms are 0, and diagonal terms are eigen values corresponding to eigen vectors in the columns of \\\\(V\\\\). Therefore, \\\\( A = V \\Lambda V^{-1}\\\\) or \\\\(\\Lambda = V^{-1} A V\\\\). In the special case where \\\\(V\\\\) is unitary, \\\\( A = V \\Lambda V^T\\\\) or \\\\( \\Lambda = V^T A V\\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Properties of eigen vectors and eigen values\n",
    "\n",
    "1. Eigen vectors are defined only for square matrices. For non-square matrices, singular value decomposition, or [SVD](http://tinyurl.com/qabkoob) is used. \n",
    "2. It is possible for a matrix to have repeated eigen values. \n",
    "3. Matrix multiplication transforms data in such a way that directions along larger eigen values are magnified. \n",
    "4. The space spanned by all the eigen vectors whose eigen values are non-zero is also the column space of the matrix.\n",
    "5. If an eigen value is 0, then its corresponding eigen vector is called a null vector. The space spanned by all the null vectors is called the null space of the matrix. \n",
    "6. If any of the eigen values is equal to 0, the matrix is not full rank. Infact, the rank of a square matrix is equal to the number of non-zero eigen values. \n",
    "7. The determinant of a matrix is equal to product of the eigen values. \n",
    "8. Eigen values represent how much the data will get skewed by multiplying it with the matrix A. The eigen vectors corresponding to largest eigen value become dominant while the smaller eigen values' vectors diminish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eigen value example \n",
    "\n",
    "Consider the matrix, \n",
    "\n",
    "$$ A = \\left[ \\begin{array}{ccc}  1.5 & 1 \\\\ 0 & 0.5 \\end{array} \\right]$$\n",
    "\n",
    "1. Find eigen values of A\n",
    "2.  What will happen to a vector when it is multiplied by \\\\( A \\\\)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Eigen value example \n",
    "$$ A = \\left[ \\begin{array}{ccc}  1.5 & 1 \\\\ 0 & 0.5 \\end{array} \\right]$$\n",
    "\n",
    "1. Find eigen values of A, 0.5 and 1.5\n",
    "2. What will happen to a vector when it is multiplied by \\\\( A \\\\)? \n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/eigen_val_Ax.png\",width=200, height=200)>\n",
    "  <figcaption> Fig1. - Determinant of a 2-dimension matrix is equal to the area of the parallelogram enclosed by the rows (or columns) of the matrix.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Properties of eigen values and eigen vectors. \n",
    "\n",
    "1. Trace of a matrix (sum of all diagonal elements) is equal to the sum of eigen values\n",
    "2. Determinant of a matrix is equal to the product of eigen values. \n",
    "3. A set of eigenvectors of \\\\(A\\\\), each corresponding to a different eigenvalue of \\\\(A\\\\), is a linearly independent set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singular value decomposition (SVD)\n",
    "\n",
    "Singular value decomposition or SVD is a powerful matrix factorization or decomposition technique. In SVD, a \\\\( n \\times m\\\\) matrix \\\\(A\\\\) is decomposed as \\\\( U \\Sigma V^{\\*} \\\\), \n",
    "\n",
    "$$ A = U \\Sigma V^{\\*} $$ \n",
    "\n",
    "where  \\\\(U\\\\), \\\\( \\Sigma \\\\) and \\\\(  V \\\\) satisfy\n",
    "\n",
    "1. \\\\( U \\\\) is called the matrix of left singular vector, and its columns are eigen vectors of \\\\( A A^{\\*} \\\\).\n",
    "2. \\\\( V \\\\) is called the matrix of right singular vector, and its columns are eigen vectors of \\\\( A^{\\*} A \\\\).\n",
    "3. \\\\( \\Sigma \\\\) is a \\\\(n \\times m \\\\) matrix whose diagonal elements are square root of eigen values of \\\\( A A^{\\*} \\\\) or \\\\( A^{\\*} A \\\\). \n",
    "4. For most matrices \\\\( n \\neq m \\\\), therefore, the maximum rank \\\\(A\\\\) can have is the lower of \\\\( m \\\\) or \\\\( n \\\\). \n",
    "5. If \\\\( n > m \\\\), then \\\\(A \\\\) has more rows than columns and \\\\( \\Sigma  \\\\) is a matrix of size \\\\( n \\times m \\\\) and its \\\\( i,i \\\\) element is square root of eigen value of \\\\( A A^{\\*} \\\\) or \\\\( A^{\\*} A \\\\) for \\\\( i = 1\\\\) to \\\\(m\\\\).\n",
    "5. If \\\\( n < m \\\\), then \\\\(A \\\\) has more columns than rows and \\\\( \\Sigma  \\\\) is a matrix of size \\\\( n \\times m \\\\) and its \\\\( i,i \\\\) element is square root of eigen value of \\\\( A A^{\\*} \\\\) or \\\\( A^{\\*} A \\\\) for \\\\( i = 1\\\\) to \\\\(n\\\\).\n",
    "\n",
    "\n",
    "\n",
    "Note: \\\\( V^{\\*} \\\\) is complex conjugate of \\\\(V\\\\). If all elements of \\\\(V\\\\) are real then \\\\(V^{*} = V^T\\\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Applications of SVD\n",
    "\n",
    "- SVD is a powerful dimensionality reduction technique. \n",
    "\n",
    "- The eigen vectors of covariance matrix also represent the percentage of explained variance in data. \n",
    "- Therefore, square of singular values of a matrix represent the relative amount of data explained by the corresponding eigen vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### SVD example 1\n",
    "\n",
    "Raw_signals\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/Raw_signals.png\",width=200, height=200)>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### SVD example 1\n",
    "\n",
    "Raw_signals reconstructed using first 4 components, see [notes for code and implementation details](https://mec560sbu.github.io/2016/08/29/LinAlg_Review_Mbook/). Savings of 90% in storage by losing 1% accuracy. \n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/Raw_signals_recons.png\",width=200, height=200)>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SVD example 2\n",
    "\n",
    "SVD for image compression. SVD can be used for image compression too. \n",
    "- Images are matrix of numbers\n",
    "- Images typically have many correlated features. \n",
    "- Images of size $N \\times N$ have storage requirement of $N^2$\n",
    "- If you take selected few components, the storage requirements drop.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/HangingLake.png\",width=200, height=200)>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVD example 2\n",
    "\n",
    "SVD for image compression. SVD can be used for image compression too. \n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/img_recons.png\",width=200, height=200)>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
